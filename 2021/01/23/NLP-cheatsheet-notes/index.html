<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NLP佛脚笔记 | Dave是只学习基</title><meta name="description" content="（施工中……） 复制粘贴为主 比较简单只是为了定性地熟悉和回忆一下相关知识 但是如果有大佬发现了低级错误 请不吝在评论区中及时指出 授人玫瑰🌹手留余香 感恩❤️ 主要参考资料：https:&#x2F;&#x2F;github.com&#x2F;NLP-LOVE&#x2F;ML-NLP  Machine Learning &#x2F; Deep Learning部分基本概念激活函数：sigmoid，Tanh，ReLU，Leaky ReLU，PRe"><meta name="keywords" content="NLP"><meta name="author" content="Dawei Jin,davekim.dawei@gmail.com"><meta name="copyright" content="Dawei Jin"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/DaveKim3872/blogpics/pics128-dog.png"><link rel="canonical" href="https://davekim3872.github.io/2021/01/23/NLP-cheatsheet-notes/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="preconnect" href="//zz.bdstatic.com"/><meta property="og:type" content="article"><meta property="og:title" content="NLP佛脚笔记"><meta property="og:url" content="https://davekim3872.github.io/2021/01/23/NLP-cheatsheet-notes/"><meta property="og:site_name" content="Dave是只学习基"><meta property="og:description" content="（施工中……） 复制粘贴为主 比较简单只是为了定性地熟悉和回忆一下相关知识 但是如果有大佬发现了低级错误 请不吝在评论区中及时指出 授人玫瑰🌹手留余香 感恩❤️ 主要参考资料：https:&#x2F;&#x2F;github.com&#x2F;NLP-LOVE&#x2F;ML-NLP  Machine Learning &#x2F; Deep Learning部分基本概念激活函数：sigmoid，Tanh，ReLU，Leaky ReLU，PRe"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/josh-miller-x0jaoF-qPCU-unsplash.jpg"><meta property="article:published_time" content="2021-01-23T11:21:20.000Z"><meta property="article:modified_time" content="2021-01-25T06:23:21.784Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'true'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?00e5092a8a15b5956d9a3c699b0b9723";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-175349428-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-175349428-1');
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: {"limitCount":50,"languages":{"author":"作者: Dawei Jin","link":"链接: ","source":"来源: Dave是只学习基","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: true,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2021-01-25 14:23:21'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><meta name="baidu-site-verification" content="XVkIO6oR4Q" /><meta name="google-site-verification" content="mP9y14KnjgEYzIjDaOzkQ4bKEVJm2vdi1s6lk9pQRQI" /><meta name="generator" content="Hexo 5.0.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" data-lazy-src="https://cdn.jsdelivr.net/gh/DaveKim3872/blogpics/WechatIMG28.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">16</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">26</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 图片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-lightbulb"></i><span> 关于</span></a></div></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Machine-Learning-Deep-Learning%E9%83%A8%E5%88%86"><span class="toc-number">1.</span> <span class="toc-text">Machine Learning &#x2F; Deep Learning部分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9Asigmoid%EF%BC%8CTanh%EF%BC%8CReLU%EF%BC%8CLeaky-ReLU%EF%BC%8CPReLU%EF%BC%8CELU%EF%BC%8CMaxout"><span class="toc-number">1.1.1.</span> <span class="toc-text">激活函数：sigmoid，Tanh，ReLU，Leaky ReLU，PReLU，ELU，Maxout</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#sigmoid"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">sigmoid</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Tanh"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">Tanh</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ReLU"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">ReLU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LeakyReLU"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">LeakyReLU</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5cross-entropy"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">交叉熵cross-entropy</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%85%AB%E8%82%A1"><span class="toc-number">1.2.</span> <span class="toc-text">模型八股</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN"><span class="toc-number">1.2.1.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LSTM"><span class="toc-number">1.2.2.</span> <span class="toc-text">LSTM</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NLP%E9%83%A8%E5%88%86"><span class="toc-number">2.</span> <span class="toc-text">NLP部分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-1"><span class="toc-number">2.1.</span> <span class="toc-text">基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Word-Embedding"><span class="toc-number">2.1.1.</span> <span class="toc-text">Word Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Methods"><span class="toc-number">2.1.1.1.</span> <span class="toc-text">Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#One-Hot"><span class="toc-number">2.1.1.1.1.</span> <span class="toc-text">One-Hot</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Bag-of-Words"><span class="toc-number">2.1.1.1.2.</span> <span class="toc-text">Bag of Words</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#TF-IDF"><span class="toc-number">2.1.1.1.3.</span> <span class="toc-text">TF-IDF</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#n-gram"><span class="toc-number">2.1.1.1.4.</span> <span class="toc-text">n-gram</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%85%AB%E8%82%A1-1"><span class="toc-number">2.2.</span> <span class="toc-text">模型八股</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Word2Vec"><span class="toc-number">2.2.0.1.</span> <span class="toc-text">Word2Vec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Attention"><span class="toc-number">2.2.0.2.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer"><span class="toc-number">2.2.0.3.</span> <span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ELMo-Embedding-from-Language-Models"><span class="toc-number">2.2.0.4.</span> <span class="toc-text">ELMo (Embedding from Language Models)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BERT%EF%BC%9AA-Pre-training-model"><span class="toc-number">2.2.0.5.</span> <span class="toc-text">BERT：A Pre-training model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GPT-GPT-2"><span class="toc-number">2.2.0.6.</span> <span class="toc-text">GPT &#x2F; GPT-2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#XLNET"><span class="toc-number">2.2.0.7.</span> <span class="toc-text">XLNET</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92vs%E8%87%AA%E7%BC%96%E7%A0%81"><span class="toc-number">2.2.0.7.1.</span> <span class="toc-text">自回归vs自编码</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GloVe"><span class="toc-number">2.2.0.8.</span> <span class="toc-text">GloVe</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ULMFit"><span class="toc-number">2.2.0.9.</span> <span class="toc-text">ULMFit</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">2.2.1.</span> <span class="toc-text">评价指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#F1"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">F1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BLeu"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">BLeu</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-number">2.2.1.2.1.</span> <span class="toc-text">核心思想</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9"><span class="toc-number">2.2.1.2.2.</span> <span class="toc-text">主要特点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">2.2.1.2.3.</span> <span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">2.2.1.2.4.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B"><span class="toc-number">2.2.1.2.5.</span> <span class="toc-text">改进</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rouge"><span class="toc-number">2.2.1.3.</span> <span class="toc-text">Rouge</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-1"><span class="toc-number">2.2.1.3.1.</span> <span class="toc-text">核心思想</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/josh-miller-x0jaoF-qPCU-unsplash.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Dave是只学习基</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 图片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-lightbulb"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">NLP佛脚笔记</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-01-23T11:21:20.000Z" title="发表于 2021-01-23 19:21:20">2021-01-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-01-25T06:23:21.784Z" title="更新于 2021-01-25 14:23:21">2021-01-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%9D%A2%E8%AF%95%E5%A4%8D%E4%B9%A0%E5%B0%8F%E6%8A%84/">面试复习小抄</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p>（施工中……）</p>
<p>复制粘贴为主 比较简单只是为了定性地熟悉和回忆一下相关知识 但是如果有大佬发现了低级错误 请不吝在评论区中及时指出 授人玫瑰🌹手留余香 感恩❤️</p>
<p>主要参考资料：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP">https://github.com/NLP-LOVE/ML-NLP</a> </p>
<h1 id="Machine-Learning-Deep-Learning部分"><a href="#Machine-Learning-Deep-Learning部分" class="headerlink" title="Machine Learning / Deep Learning部分"></a>Machine Learning / Deep Learning部分</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="激活函数：sigmoid，Tanh，ReLU，Leaky-ReLU，PReLU，ELU，Maxout"><a href="#激活函数：sigmoid，Tanh，ReLU，Leaky-ReLU，PReLU，ELU，Maxout" class="headerlink" title="激活函数：sigmoid，Tanh，ReLU，Leaky ReLU，PReLU，ELU，Maxout"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/71882757">激活函数：sigmoid，Tanh，ReLU，Leaky ReLU，PReLU，ELU，Maxout</a></h3><h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>sigmoid函数又称 Logistic函数，用于隐层神经元输出，取值范围为(0,1)，可以用来做二分类。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://www.zhihu.com/equation?tex=%5Csigma(x)=%5Cfrac%7B1%7D%7B1+e%5E%7B-x%7D%7D" alt="[公式]"></p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/avTdiw.jpg" alt="avTdiw"></p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/JOADpO.jpg" alt="JOADpO"></p>
<p>优点：</p>
<ul>
<li><p>Sigmoid函数的输出在(0,1)之间，输出范围有限，优化稳定，可以用作输出层。</p>
</li>
<li><p>连续函数，便于求导。</p>
</li>
</ul>
<p>缺点：</p>
<ul>
<li><p>sigmoid函数在变量取绝对值非常大的正值或负值时会出现饱和现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感。</p>
</li>
<li><p>在反向传播时，当梯度接近于0，权重基本不会更新，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。</p>
</li>
<li><p>sigmoid函数的输出不是0均值的，会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。</p>
</li>
<li><p>计算复杂度高，因为sigmoid函数是指数形式。</p>
</li>
</ul>
<h4 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h4><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/PbkLCR.jpg" alt="PbkLCR"></p>
<h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p><strong>整流线性单元</strong>（Rectified linear unit，ReLU）是现代神经网络中最常用的激活函数，大多数前馈神经网络默认使用的激活函数。</p>
<p>ReLU函数定义如下： <img src= "/img/loading.gif" data-lazy-src="https://www.zhihu.com/equation?tex=f(x)=max(0,x)" alt="[公式]"></p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/jLl62p.jpg" alt="jLl62p"></p>
<p><strong>优点：</strong></p>
<p>使用ReLU的SGD算法的收敛速度比 sigmoid 和 tanh 快。</p>
<p>在x&gt;0区域上，不会出现梯度饱和、梯度消失的问题。</p>
<p>计算复杂度低，不需要进行指数运算，只要一个阈值就可以得到激活值。</p>
<p><strong>缺点：</strong></p>
<p>ReLU的输出<strong>不是0均值</strong>的。</p>
<p>**Dead ReLU Problem(神经元坏死现象)**：ReLU在负数区域被kill的现象叫做dead relu。ReLU在训练的时很“脆弱”。在x&lt;0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新。</p>
<p><strong>产生</strong>这种现象的两个<strong>原因</strong>：参数初始化问题；learning rate太高导致在训练过程中参数更新太大。 </p>
<p><strong>解决方法</strong>：采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</p>
<h4 id="LeakyReLU"><a href="#LeakyReLU" class="headerlink" title="LeakyReLU"></a>LeakyReLU</h4><p><strong>渗漏整流线性单元</strong>(Leaky ReLU)，为了解决dead ReLU现象。用一个类似0.01的小值来初始化神经元，从而<strong>使得ReLU在负数区域更偏向于激活而不是死掉</strong>。这里的斜率都是确定的。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/nIEhNp.jpg" alt="nIEhNp"></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><h4 id="交叉熵cross-entropy"><a href="#交叉熵cross-entropy" class="headerlink" title="交叉熵cross-entropy"></a>交叉熵cross-entropy</h4><p><img src= "/img/loading.gif" data-lazy-src="https://www.zhihu.com/equation?tex=L+=+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D+L_i+=+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D-%5By_i%5Ccdot+log(p_i)+++(1-y_i)%5Ccdot+log(1-p_i)%5D+%5C%5C" alt="[公式]"></p>
<p><img src= "/img/loading.gif" data-lazy-src="https://www.zhihu.com/equation?tex=L+=+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D+L_i+=+%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%7D+-%5Csum_%7Bc=1%7D%5EMy_%7Bic%7D%5Clog(p_%7Bic%7D)+%5C%5C" alt="[公式]"></p>
<p>当使用sigmoid作为激活函数的时候，常用<strong>交叉熵损失函数</strong>而不用<strong>均方误差损失函数</strong>，因为它可以<strong>完美解决平方损失函数权重更新过慢</strong>的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。</p>
<h2 id="模型八股"><a href="#模型八股" class="headerlink" title="模型八股"></a>模型八股</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>结构：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/Itl4lS.jpg" alt="Itl4lS"></p>
<p>缺点：Gradient Vanishing, No Long-distance dependency</p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>结构：</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/bSUX3M.jpg" alt="bSUX3M"></p>
<p>其中，f代表forget，对上一状态的c(t-1)进行选择性遗忘，i代表对x(t)输入后的候选记忆细胞c’(t)进行选择性记忆。这两个结果相加再经过一层o（决定哪些会被输出）构成了真正的当前状态c(t)的隐藏状态h(t)。</p>
<p><strong>记忆门：</strong><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/QpE9hp.jpg" alt="QpE9hp"></p>
<p><strong>遗忘门：</strong><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/3SIJu6.jpg" alt="3SIJu6"></p>
<p><strong>输出门：</strong><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/tYsOzX.jpg" alt="tYsOzX"></p>
<p><strong>候选记忆细胞：</strong><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/VYjN0N.jpg" alt="VYjN0N"></p>
<p><strong>记忆细胞</strong>：<img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/qhhBRW.jpg" alt="qhhBRW"></p>
<p><strong>隐藏状态：</strong><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/Ep0OYF.jpg" alt="Ep0OYF"></p>
<p>优缺点：with long-distance dependency, but more difficult to train(more parameters)</p>
<h1 id="NLP部分"><a href="#NLP部分" class="headerlink" title="NLP部分"></a>NLP部分</h1><h2 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h3><p>Use vectors to represent words 把词映射为实数域向量的技术</p>
<h4 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h4><h5 id="One-Hot"><a href="#One-Hot" class="headerlink" title="One-Hot"></a>One-Hot</h5><p>Pros: Expand the dimension of features</p>
<p>Cons: 随着语料库的增加，数据特征的维度会越来越大，产生一个维度很高，又很稀疏的矩阵。这种表示方法的分词顺序和在句子中的顺序是无关的，不能保留词与词之间的关系信息。</p>
<h5 id="Bag-of-Words"><a href="#Bag-of-Words" class="headerlink" title="Bag of Words"></a>Bag of Words</h5><p>Cons: 词出现的越多特征越明显，但不代表这个词重要/权重应该更大。词与词之间的顺序关系被抹除。</p>
<h5 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h5><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/rpP7Sr.jpg" alt="rpP7Sr"></p>
<p>含义：某个term在一篇文章中出现的频率越高 &amp; 这个term在所有文档中出现的次数越少，越能够代表这个文章。TFIDF方法主要是对词向量进行加权。</p>
<h5 id="n-gram"><a href="#n-gram" class="headerlink" title="n-gram"></a>n-gram</h5><p>Cons：随着n的大小增大，词表会呈现指数型膨胀。</p>
<h2 id="模型八股-1"><a href="#模型八股-1" class="headerlink" title="模型八股"></a>模型八股</h2><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.1%20Word%20Embedding#42-word2vec">Word2Vec</a></h4><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/0LXo0u.jpg" alt="0LXo0u"></p>
<p>本质：训练上下文预测的神经网络模型，利用隐藏层权重矩阵作为向量化表示。所需要的结果是输入层和隐藏层之间的权重矩阵W，用输入乘以W就是词向量表示。</p>
<p>方式：CBOW（连续词袋，预测下文）or Skip-gram（根据上下文预测词语）</p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>结构： </p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/ReyEHg.jpg" alt="ReyEHg"></p>
<p>利用i-1的隐藏层输出计算概率分布，softmax归一化</p>
<h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><p>结构：Transformer的结构和Attention模型一样，Transformer模型中也采用了 encoer-decoder 架构。但其结构相比于Attention更加复杂，论文中encoder层由6个encoder堆叠在一起，decoder层也一样。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/XR389N.jpg" alt="XR389N"></p>
<ul>
<li>encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。</li>
<li>decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。</li>
</ul>
<h4 id="ELMo-Embedding-from-Language-Models"><a href="#ELMo-Embedding-from-Language-Models" class="headerlink" title="ELMo (Embedding from Language Models)"></a>ELMo (Embedding from Language Models)</h4><p>For what : Word Embedding 无法解决同义词（Synonym）问题</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/D6H9k9.jpg" alt="D6H9k9"></p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/WcecY6.png" alt="WcecY6"></p>
<h4 id="BERT：A-Pre-training-model"><a href="#BERT：A-Pre-training-model" class="headerlink" title="BERT：A Pre-training model"></a>BERT：A Pre-training model</h4><p>结构</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/P2stYh.jpg" alt="P2stYh"></p>
<p>BERT预训练模型分为以下三个步骤：<strong>Embedding、Masked LM、Next Sentence Prediction</strong></p>
<ul>
<li><p>Embedding：这里的Embedding由三种Embedding求和而成</p>
<ul>
<li>Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务</li>
<li>Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务</li>
<li>Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的</li>
</ul>
</li>
<li><p>MLM可以理解为完形填空，作者会随机mask每一个句子中15%的词，用其上下文来做预测，例如：my dog is hairy → my dog is [MASK]</p>
<p>此处将hairy进行了mask处理，然后采用非监督学习的方法预测mask位置的词是什么，但是该方法有一个问题，因为是mask15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过，为了解决这个问题，作者做了如下的处理：</p>
<p>80%是采用[mask]，my dog is hairy → my dog is [MASK]</p>
<p>10%是随机取一个词来代替mask的词，my dog is hairy -&gt; my dog is apple</p>
<p>10%保持不变，my dog is hairy -&gt; my dog is hairy</p>
</li>
<li><p>选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。 个人理解：</p>
<ul>
<li>Bert先是用Mask来提高视野范围的信息获取量，增加duplicate再随机Mask，这样跟RNN类方法依次训练预测没什么区别了除了mask不同位置外；</li>
<li>全局视野极大地降低了学习的难度，然后再用A+B/C来作为样本，这样每条样本都有50%的概率看到一半左右的噪声；</li>
<li>但直接学习Mask A+B/C是没法学习的，因为不知道哪些是噪声，所以又加上next_sentence预测任务，与MLM同时进行训练，这样用next来辅助模型对噪声/非噪声的辨识，用MLM来完成语义的大部分的学习。</li>
</ul>
</li>
</ul>
<h4 id="GPT-GPT-2"><a href="#GPT-GPT-2" class="headerlink" title="GPT / GPT-2"></a>GPT / GPT-2</h4><h4 id="XLNET"><a href="#XLNET" class="headerlink" title="XLNET"></a>XLNET</h4><h5 id="自回归vs自编码"><a href="#自回归vs自编码" class="headerlink" title="自回归vs自编码"></a>自回归vs自编码</h5><ul>
<li><p><strong>Autoregressive LM:</strong> 在ELMO／BERT出来之前，大家通常讲的语言模型其实是根据上文内容预测下一个可能跟随的单词，就是常说的自左向右的语言模型任务，或者反过来也行，就是根据下文预测前面的单词，这种类型的LM被称为自回归语言模型。GPT 就是典型的自回归语言模型。ELMO尽管看上去利用了上文，也利用了下文，但是本质上仍然是自回归LM，这个跟模型具体怎么实现有关系。ELMO是做了两个方向（从左到右以及从右到左两个方向的语言模型），但是是分别有两个方向的自回归LM，然后把LSTM的两个方向的隐节点状态拼接到一起，来体现双向语言模型这个事情的。所以其实是两个自回归语言模型的拼接，本质上仍然是自回归语言模型。</p>
<p>自回归语言模型有优点有缺点：</p>
<p><strong>缺点</strong>是只能利用上文或者下文的信息，不能同时利用上文和下文的信息，当然，貌似ELMO这种双向都做，然后拼接看上去能够解决这个问题，因为融合模式过于简单，所以效果其实并不是太好。</p>
<p><strong>优点</strong>其实跟下游NLP任务有关，比如生成类NLP任务，比如文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。而Bert这种DAE模式，在生成类NLP任务中，就面临训练过程和应用过程不一致的问题，导致生成类的NLP任务到目前为止都做不太好。</p>
</li>
<li><p><strong>Autoencoder:</strong> 自回归语言模型只能根据上文预测下一个单词，或者反过来，只能根据下文预测前面一个单词。相比而言，Bert通过在输入X中随机Mask掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被Mask掉的单词，如果你对Denoising Autoencoder比较熟悉的话，会看出，这确实是典型的DAE的思路。那些被Mask掉的单词就是在输入侧加入的所谓噪音。类似Bert这种预训练模式，被称为DAE LM。</p>
<p>这种DAE LM的优缺点正好和自回归LM反过来，它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，这是好处。<strong>缺点</strong>是啥呢？主要在输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的。DAE吗，就要引入噪音，[Mask] 标记就是引入噪音的手段，这个正常。</p>
<p>XLNet的出发点就是：能否融合自回归LM和DAE LM两者的优点。就是说如果站在自回归LM的角度，如何引入和双向语言模型等价的效果；如果站在DAE LM的角度看，它本身是融入双向语言模型的，如何抛掉表面的那个[Mask]标记，让预训练和Fine-tuning保持一致。当然，XLNet还讲到了一个Bert被Mask单词之间相互独立的问题。</p>
</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/QrjZXO.jpg" alt="QrjZXO"></p>
<h4 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h4><p><em>共现矩阵</em>（Co-occurrence Matrix）</p>
<h4 id="ULMFit"><a href="#ULMFit" class="headerlink" title="ULMFit"></a>ULMFit</h4><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><h4 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h4><p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/c9DWK1.jpg" alt="c9DWK1"></p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/xO3RyO.jpg" alt="xO3RyO"></p>
<h4 id="BLeu"><a href="#BLeu" class="headerlink" title="BLeu"></a>BLeu</h4><h5 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h5><p>比较候选译文和参考译文里的 n-gram 的重合程度，重合程度越高就认为译文质量越高。unigram用于衡量单词翻译的准确性，高阶n-gram用于衡量句子翻译的流畅性。 实践中，通常是取N=1~4，然后对进行加权平均。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/DNyUpN.png" alt="DNyUpN"></p>
<h5 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h5><ul>
<li>n-gram共现统计</li>
<li>基于精确率</li>
</ul>
<h5 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h5><p>Machine Translation</p>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li><p>只看重精确率，不看重召回率。</p>
</li>
<li><p>存在常用词干扰（可以用截断的方法解决）</p>
</li>
<li><p>短句得分较高。即使引入了brevity penalty，也还是不够。</p>
</li>
</ul>
<h5 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h5><ul>
<li>截断：改进常用词干扰</li>
<li>brevity penalty：改进短句得分较高的问题</li>
</ul>
<h4 id="Rouge"><a href="#Rouge" class="headerlink" title="Rouge"></a>Rouge</h4><h5 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h5><p>大致分为四种：ROUGE-N，ROUGE-L，ROUGE-W，ROUGE-S。常用的是前两种（-N与-L） * ROUGE-N中的“N”指的是N-gram，其计算方式与BLEU类似，只是BLEU基于精确率，而ROUGE基于召回率。</p>
<ul>
<li>ROUGE-L中的“L”指的是Longest Common Subsequence，计算的是候选摘要与参考摘要的最长公共子序列长度，长度越长，得分越高，基于F值。</li>
</ul>
<p>![image-20210125134654580](/Users/dawei/Library/Application Support/typora-user-images/image-20210125134654580.png)</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:davekim.dawei@gmail.com">Dawei Jin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://davekim3872.github.io/2021/01/23/NLP-cheatsheet-notes/">https://davekim3872.github.io/2021/01/23/NLP-cheatsheet-notes/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://davekim3872.github.io" target="_blank">Dave是只学习基</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/josh-miller-x0jaoF-qPCU-unsplash.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/DaveKim3872/blogpics/wechat-pay.jpg" target="_blank"><img class="post-qr-code-img" data-lazy-src="https://cdn.jsdelivr.net/gh/DaveKim3872/blogpics/wechat-pay.jpg" alt="微信Wechat"/></a><div class="post-qr-code-desc">微信Wechat</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/DaveKim3872/blogpics/Alipay.jpg" target="_blank"><img class="post-qr-code-img" data-lazy-src="https://cdn.jsdelivr.net/gh/DaveKim3872/blogpics/Alipay.jpg" alt="支付宝Alipay"/></a><div class="post-qr-code-desc">支付宝Alipay</div></li></ul></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/07/09/Leetcode-31-next-permutation/"><img class="prev-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/DaveKim3872/blogpics/lcph_logo_fit.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Leetcode 31. 下一个排列</div></div></a></div><div class="next-post pull-right"><a href="/2020/08/31/pyspark-use-scala-udf/"><img class="next-cover" data-lazy-src="https://cdn.jsdelivr.net/gh/DaveKim3872/blogpics/20200831110438.png" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">PySpark调用Scala UDF</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><label><input id="switch-comments-btn" type="checkbox"/><span class="slider"></span></label><span class="second-comment">Livere</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="lv-container" data-id="city" data-uid="MTAyMC81MTMxMS8yNzc5Mg=="></div></div></div></div></article></main><footer id="footer" style="background-image: url(https://cdn.jsdelivr.net/gh/davekim3872/blogpics@master/uPic/josh-miller-x0jaoF-qPCU-unsplash.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2021 By Dawei Jin</div><div class="footer_custom_text">哎一古！金社长</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    window.valine = new Valine({
      el: '#vcomment',
      appId: 'VWnNfzUoHTUE37IzEwnODdyg-gzGzoHsz',
      appKey: 'KEBrPQBnoyUoKpqgnKLNe74x',
      placeholder: '在上面留下邮箱可以第一时间收到回复 😁',
      avatar: 'wavatar',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
    });
    if ('nick,mail') { valine.config.requiredFields= 'nick,mail'.split(',') }
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Valine' === 'Livere' || !true) {
  if (true) loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script></div></body></html>